{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis of Multiversion Concurrency Control Conflict in Hyperledger Fabric\n",
    "In Hyperledger-Fabric each transaction is first executed, then ordered by data dependencies, then validated against the current state of the blockchain. Because of this architecture transactions can be executed in parallel, however if a key is written to, its version  number is updated and all transactions that have not yet been validated and read the previous version number, get invalidated. They receive the validation code of MVCC_READ_CONFILCT.\n",
    "\n",
    "If a lot of transactions receive this code, it will negatively affect performance of the system. This notebook aims to uncover the patterns that cause a lot of these conflicts, so the chaincode can be rewritten in a way that avoids this.\n",
    "\n",
    "This notebook depends on a running explorer database. With the provided shell script, a database can be run locally, with the sample data provided in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "# import bamboolib as bam\n",
    "import plotly.express as px\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"iframe\"\n",
    "import networkx as nx\n",
    "import swifter\n",
    "from graphdatascience import GraphDataScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract the asset type from the composite-keys\n",
    "def getAssetType(key):\n",
    "    parts=key.split(\"||\")\n",
    "    if len(parts)>1:\n",
    "        return parts[1]\n",
    "    return parts[0]\n",
    "# function to assign -1 to mvccs, +1 to valid, 0 to all else\n",
    "def gradeValidation(code):\n",
    "    if code=='MVCC_READ_CONFLICT':\n",
    "        return -1\n",
    "    elif code=='VALID':\n",
    "        return +1\n",
    "    else:\n",
    "        return 0\n",
    "# function to find keys in the rwsets' jsons\n",
    "def findKeys(string):\n",
    "    pat = r\"(?<='key': ').+?(?=')\"\n",
    "    return re.findall(pat,string)\n",
    "# function to simulate complex keys, if the keys in the chaincode represent different asset types, but complex keys were not used\n",
    "def simulateCompKeys(key,asset_type_prefix):\n",
    "    k = str(key)\n",
    "    if(asset_type_prefix in k):\n",
    "        parts = k.split(asset_type_prefix)\n",
    "        return '\\x00' + k[0]+ asset_type_prefix + '\\x00' + parts[1] + '\\x00'\n",
    "    else:\n",
    "        return str(key)\n",
    "# function to determine the tx level in a block\n",
    "def blockLevelTxNum(row):\n",
    "    if(row['blockid'] ==row['prev_blockid']):\n",
    "        return row['prev_lvl']+1\n",
    "    return 0\n",
    "# function to check if a column contains a series of integers without missing any between the min and max \n",
    "def continuityCheck(df,columname):\n",
    "    tmp_df = df.sort_values(by=columname,ascending=True)\n",
    "    check = {'col':tmp_df[columname],'check':tmp_df[columname].diff()}\n",
    "    check = pd.DataFrame(check)\n",
    "    check = check.loc[check['check']>abs(1)]\n",
    "    missing = check['check'].sum()\n",
    "    print(\"Total missing from \" + columname + \": \" + str(missing))\n",
    "    return check\n",
    "# function to delete all data from graph database\n",
    "def deleteGraphDbContent():\n",
    "    gds = GraphDataScience(\"bolt://localhost:7687\", auth=None)\n",
    "    res = gds.run_cypher(\n",
    "    \"\"\"\n",
    "    MATCH (n) DETACH DELETE n\n",
    "    \"\"\"\n",
    "    )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATABASE_HOST=\"172.20.0.2\"\n",
    "#  vm.niif.cloud.bme.hu:18971 \n",
    "# DATABASE_HOST=\"vm.niif.cloud.bme.hu\"\n",
    "# DATABASE_PORT=\":18971/\"\n",
    "DATABASE_PORT=\":5432/\"\n",
    "DATABASE_DATABASE=\"fabricexplorer\"\n",
    "DATABASE_USERNAME=\"hppoc\"\n",
    "DATABASE_PASSWORD=\"password\"\n",
    "\n",
    "# pio.renderers.default='notebook'\n",
    "\n",
    "# list all user defined tables and schemas\n",
    "# \"SELECT * FROM pg_catalog.pg_tables WHERE schemaname != 'information_schema' AND schemaname != 'pg_catalog';\"\n",
    "\n",
    "#initializing resources\n",
    "engine = create_engine('postgresql://'+DATABASE_USERNAME+\n",
    "                       ':'+DATABASE_PASSWORD+'@'\n",
    "                       +DATABASE_HOST\n",
    "                       +DATABASE_PORT\n",
    "                       +DATABASE_DATABASE)\n",
    "\n",
    "# get all transactions from database\n",
    "# txQuery = \"SELECT * FROM transactions\"\n",
    "txQuery = \"SELECT * FROM transactions ORDER BY id DESC limit 4000\"\n",
    "txDf = pd.read_sql(txQuery,con=engine)\n",
    "# check if all data was recorded by explorer\n",
    "continuityCheck(txDf,\"blockid\")\n",
    "continuityCheck(txDf,\"id\")\n",
    "blockQuery = \"SELECT * FROM blocks\"\n",
    "blockDf = pd.read_sql(blockQuery,con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start tidying data, drop the transactions without readsets\n",
    "txDf=txDf[txDf['read_set'].isna()==False]\n",
    "# drop the ones committed by lifecycle chaincode\n",
    "txDf=txDf[txDf['chaincodename'].str.contains('_lifecycle')==False]\n",
    "# drop the config transactions\n",
    "txDf=txDf[txDf['type'].str.contains('CONFIG')==False]\n",
    "# convert validation code to string type\n",
    "txDf['validation_code']=txDf['validation_code'].astype('string')\n",
    "# drop a bunch of unneeded coloumns\n",
    "timeDf=txDf.drop(columns=[\"txhash\",\"chaincode_id\",\"endorser_msp_id\",\"type\",\"channel_genesis_hash\",\"envelope_signature\",\"creator_id_bytes\", \"creator_nonce\", \"payload_extension\",\"tx_response\",\"payload_proposal_hash\",\"endorser_id_bytes\",\"endorser_signature\", \"network_name\",\"chaincode_proposal_input\",\"read_set\",\"write_set\"])\n",
    "transactions = txDf.drop(columns=[\"txhash\",\"chaincode_id\",\"endorser_msp_id\",\"type\",\"channel_genesis_hash\",\"envelope_signature\",\"creator_id_bytes\", \"creator_nonce\", \"payload_extension\",\"tx_response\",\"payload_proposal_hash\",\"endorser_id_bytes\",\"endorser_signature\", \"network_name\",\"chaincode_proposal_input\",])\n",
    "txDf=txDf.drop(columns=[\"txhash\",\"chaincode_id\",\"endorser_msp_id\",\"type\",\"createdt\",\"channel_genesis_hash\",\"envelope_signature\",\"creator_id_bytes\", \"creator_nonce\", \"payload_extension\",\"tx_response\",\"payload_proposal_hash\",\"endorser_id_bytes\",\"endorser_signature\", \"network_name\"])\n",
    "# add default tx level in blocks to all transactions \n",
    "txDf['tx_level']=0\n",
    "prev_id = 0\n",
    "prev_lvl = 0\n",
    "# sort transactions by id ascending\n",
    "txDf = txDf.sort_values(by=['id'],ascending=True)\n",
    "# shift the blockId up by one as previous blockid\n",
    "txDf['prev_blockid']=txDf['blockid'].shift(1).fillna(0).astype(int)\n",
    "# shift the tx level up by one as previous level\n",
    "txDf['prev_lvl'] = txDf['tx_level'].shift(1).fillna(0).astype(int)\n",
    "# determine tx level in blocks\n",
    "txDf['tx_level'] = txDf.apply(blockLevelTxNum,axis=1)\n",
    "# drop helper columns\n",
    "txDf = txDf.drop(columns=['prev_lvl','prev_blockid'])\n",
    "# drop lifecycle chaincodes\n",
    "txDf = txDf[~((txDf['chaincodename'].str.contains('_lifecycle'))|(txDf['chaincodename'].str.contains('lscc')))]\n",
    "# get all distinct chaincodes\n",
    "distinct_cc = txDf['chaincodename'].unique()\n",
    "continuityCheck(txDf,\"blockid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rows from each read set\n",
    "txDf = txDf.explode('read_set')\n",
    "# create rows from each write set\n",
    "txDf = txDf.explode('write_set')\n",
    "# drop transactions with readsets and writest created by lifecycle chaincodes\n",
    "txDf = txDf[~((txDf['read_set'].astype('string').str.contains(\"lscc\"))|(txDf['read_set'].astype('string').str.contains(\"_lifecycle\")))]\n",
    "txDf = txDf[~((txDf['write_set'].astype('string').str.contains(\"lscc\"))|(txDf['write_set'].astype('string').str.contains(\"_lifecycle\")))]\n",
    "# create separate dataframes for read and write keyaccesses\n",
    "txDf_reads = txDf.drop(columns=['write_set'])\n",
    "txDf_writes = txDf.drop(columns=['read_set'])\n",
    "# add category information as access type\n",
    "txDf_reads['access_type'] = 'READ'\n",
    "txDf_writes['access_type'] = 'WRITE'\n",
    "# find all written keys in write sets with regex\n",
    "txDf_writes['keys']=txDf_writes['write_set'].astype('string').apply(lambda x: findKeys(x))\n",
    "# write_set no longer needed, dropped\n",
    "txDf_writes = txDf_writes.drop(columns=['write_set'])\n",
    "# create rows from all keys in the list extracted from the write sets and rename column\n",
    "txDf_writes = txDf_writes.explode('keys').rename(columns={'keys':'key'})\n",
    "# drop null values (none should occur)\n",
    "txDf_writes = txDf_writes[~txDf_writes['key'].isna()]\n",
    "# extract keys and versions from the readsets' jsons\n",
    "txDf_reads['read_set'] =txDf_reads['read_set'].apply(lambda x: x['set'])\n",
    "txDf_reads = txDf_reads.explode('read_set')\n",
    "txDf_reads = txDf_reads.dropna(subset=['read_set'])\n",
    "txDf_reads['key'] =txDf_reads['read_set'].apply(lambda x: findKeys(str(x))[0])\n",
    "# drop empty readsets\n",
    "txDf_reads = txDf_reads.loc[(txDf_reads['read_set'].astype('string').str.contains('version', case=False, regex=False, na=False))]\n",
    "txDf_reads['version_block'] =txDf_reads['read_set'].apply(lambda x:x['version']['block_num']['low'])\n",
    "txDf_reads['version_tx'] =txDf_reads['read_set'].apply(lambda x: x['version']['tx_num']['low'])\n",
    "# drop read_set, no longer needed\n",
    "txDf_reads = txDf_reads.drop(columns=['read_set'])\n",
    "# append writes to reads, creating dataframe with all keyaccesses\n",
    "txDf = txDf_reads.append(txDf_writes).reset_index(drop=True).rename(columns={'id':'txid'})\n",
    "# replace delimiting null character in composite keys\n",
    "txDf['key'] = txDf['key'].str.replace(\"\\\\x00\",\"||\",regex=False)\n",
    "# add block version information to write keyaccessess, based on the version they update the key to\n",
    "txDf['version_block'] = txDf.apply(lambda x: x['version_block']if x['access_type']=='READ'else x['blockid'],axis=1).astype(int)\n",
    "txDf['version_tx'] = txDf.apply(lambda x: x['version_tx']if x['access_type']=='READ'else x['tx_level'],axis=1).astype(int)\n",
    "continuityCheck(txDf,'blockid')\n",
    "txDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keys and the number of reads on each of them\n",
    "Seeing a lot of reads on a key might not be concerning, because if it is not written often, it likely won't cause a lot of MVCCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df containing reads only\n",
    "txDf_reads=txDf[txDf['access_type']=='READ']\n",
    "labels={\"x\":\"Read keys\",\"y\":\"Number of reads\"}\n",
    "# group by keys and count the occurrences\n",
    "reads= txDf_reads.groupby(['key']).size()\n",
    "reads = pd.DataFrame(reads)\n",
    "reads.columns = [str(column) for column in reads.columns]\n",
    "reads = reads.reset_index()\n",
    "# sort keys by read amounts descending\n",
    "reads = reads.sort_values(by=['0'], ascending=[False])\n",
    "# plot the first 10 keys\n",
    "fig = px.bar(reads.head(10), x='key', y='0', labels={'0':'reads'},title=\"Number of reads on each key\")\n",
    "#comment out this line to see the graph in log scale\n",
    "# fig.update_yaxes(type='log')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keys and the number of writes on each of them\n",
    "Seeing a lot of writes on a key might not be concerning, because if it is not read often, it likely won't cause  a lot of MVCCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df containing writes only\n",
    "txD_reads=txDf[txDf['access_type']=='WRITE']\n",
    "labels={\"x\":\"Written keys\",\"y\":\"Number of writes\"}\n",
    "# group by keys and count the occurrences\n",
    "writes = txD_reads.groupby(['key']).size()\n",
    "writes = pd.DataFrame(writes)\n",
    "writes.columns = [str(column) for column in writes.columns]\n",
    "writes = writes.reset_index()\n",
    "# sort keys by write amounts descending\n",
    "writes = writes.sort_values(by=['0'], ascending=[False])\n",
    "# plot the first 10 keys\n",
    "fig = px.bar(writes.head(10), x='key', y='0', labels={'0':'writes'},title=\"Number of writes on each key\")\n",
    "#comment out this line to see the graph in log scale\n",
    "# fig.update_yaxes(type='log')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative sum of valid txs - mvcc txs\n",
    "If all goes well, this graph only shows a monotonically increasing line. However, it is great to visualize waves of mvcc conflicts, which will be visible as the slope of the graph will decrease, or even turn negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df containing transactions sorted by tx ids\n",
    "transactions = transactions.sort_values(by=['id'], ascending=[True])\n",
    "# add new column to df and assign values to transactions based on validation code\n",
    "transactions['delta']= transactions['validation_code'].apply(lambda x: gradeValidation(x))\n",
    "# extract keys from rwsets' jsons\n",
    "transactions['read_keys'] = transactions['read_set'].apply(lambda x: findKeys(str(x)))\n",
    "transactions['written_keys'] = transactions['write_set'].apply(lambda x: findKeys(str(x)))\n",
    "# calculate the cumulative sum of the values assigned based on validation codes\n",
    "transactions['delta_cumsum'] = transactions['delta'].cumsum()\n",
    "# plot the data\n",
    "fig = px.line(transactions.sort_values(by=['id'], ascending=[True]),\n",
    "              x='id', y='delta_cumsum', hover_data=['read_keys', 'written_keys'],\n",
    "              title=\"Cumulative sum of the number of Valid - MVCC transactions\")\n",
    "fig.update_xaxes(title_text='transaction id')\n",
    "fig.update_yaxes(title_text='cumsum(valid-mvcc)')\n",
    "#comment out this line to see the graph in log scale\n",
    "# fig.update_yaxes(type='log')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transactions grouped by validation codes\n",
    "This graph just shows the number of transactions in each of the validation code groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels={\"x\":\"Validation code\",\"y\":\"Qty\"}\n",
    "# group transactions by validation codes, and calculate the size of each group\n",
    "codes=transactions.groupby(['validation_code']).size().reset_index().rename(columns={0:'size'})\n",
    "# sort rows by group size\n",
    "codes = codes.sort_values(by=['size'], ascending=[False])\n",
    "# plot the data\n",
    "fig = px.bar(x=codes['validation_code'], y=codes['size'],labels=labels,title=\"Transactions by validation code\")\n",
    "#comment out this line to see the graph in linear scale\n",
    "fig.update_yaxes(type='log')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your dataset contains large amounts of mvcc conflicts, please uncomment the cell below, and process the exported csv in the mvcc-finder notebook, that runs the kotlin kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add called function info to df in a way that kotlin recoginizes them as strings\n",
    "txDf['called_function']=\"'\"+txDf['chaincode_proposal_input'].str.split(\",\", n = 1, expand = True)[0]+\"'\"\n",
    "# sort by txid and access type ascending\n",
    "df = txDf.sort_values(by=['txid', 'access_type'], ascending=[True, True])\n",
    "# separate the called function from the proposal input\n",
    "txDf['called_function']=txDf['chaincode_proposal_input'].str.split(\",\", n = 1, expand = True)[0]\n",
    "# add new columns for counting mvccs caused by each keyaccess\n",
    "df['mvcc_cause']=0\n",
    "df['mvcc_caused_at']=\"\"\n",
    "df['mvcc_cause_found_for_tx']=False\n",
    "df['mvcc_cause_for_tx']=0\n",
    "df['mvcc_caused_at_for_tx']=\"\"\n",
    "# write df to disk as csv for processing with kotlin notebook\n",
    "df.to_csv('data/txsmvccs_pre.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you processed the dataframe with the kotlin notebook please dont run the next cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by txid and access type ascending, drop old index\n",
    "# df = pd.read_csv('data/txsmvccs_pre.csv')\n",
    "# function to find the mvcc causing keyaccess\n",
    "def findCause(df,row):\n",
    "    # extract info about current row\n",
    "    txId = row['txid']\n",
    "    blockId = row['blockid']\n",
    "    key = row['key']\n",
    "    version_block = row['version_block']\n",
    "    version_tx = row['version_tx']\n",
    "    access = row['access_type']\n",
    "    validation = row['validation_code']\n",
    "    # if the access type is read and validation code is mvcc\n",
    "    if validation=='MVCC_READ_CONFLICT' and access=='READ':\n",
    "        # find all valid writes for the given key, in the interval between the keyaccess\n",
    "        # and the block specified in the keyaccess' version\n",
    "        subset = df[(\n",
    "            (\n",
    "                (df['blockid']>version_block) |\n",
    "                ((df['blockid']==version_block) & (version_tx<df['version_tx']))\n",
    "            ) &\n",
    "            (df['key']==key)&\n",
    "            (df['access_type']=='WRITE')&\n",
    "            (df['validation_code']=='VALID')&\n",
    "            (df['txid']<txId)\n",
    "            )]\n",
    "        # if the subset isn't empty\n",
    "        if(len(subset)>0):\n",
    "            # found the last valid write, +1 mvccs caused by it\n",
    "            df['mvcc_cause'].iloc[[subset.tail(1).index[0]]]=df['mvcc_cause'].iloc[[subset.tail(1).index[0]]]+1\n",
    "            df['mvcc_caused_at'].iloc[[subset.tail(1).index[0]]]=df['mvcc_caused_at'].iloc[[subset.tail(1).index[0]]] + str(txId) + \",\"\n",
    "            # find out if any other key has been marked as mvcc causing in the transaction\n",
    "            tx = df[(df['txid']==txId)&(df['mvcc_cause_found_for_tx']==True)]\n",
    "            # if not mark this row as mvcc causing\n",
    "            if(len(tx)==0):\n",
    "                df['mvcc_cause_for_tx'].iloc[[subset.tail(1).index[0]]]=df['mvcc_cause_for_tx'].iloc[[subset.tail(1).index[0]]]+1\n",
    "                df['mvcc_caused_at_for_tx'].iloc[[subset.tail(1).index[0]]]=df['mvcc_caused_at_for_tx'].iloc[[subset.tail(1).index[0]]] + str(txId) + \",\"\n",
    "                df['mvcc_cause_found_for_tx'].loc[df['txid']==txId]=True\n",
    "                \n",
    "df = df.reset_index()\n",
    "df.swifter.apply(lambda x: findCause(df,x),axis=1)\n",
    "# write to disk as csv\n",
    "df.to_csv('data/txsmvccs_post_no_gb.csv')\n",
    "# group by keys and calculate the sum of mvccs they caused\n",
    "df = df.groupby(['key']).agg(mvccs_caused=('mvcc_cause', 'sum')).reset_index()\n",
    "# Keep rows where mvccs_caused > 0\n",
    "df = df.loc[df['mvccs_caused'] > 0]\n",
    "# sort rows by mvccs caused descending\n",
    "df = df.sort_values(by=['mvccs_caused'], ascending=[False])\n",
    "# os.system(\"rm data/txsmvccs_post.csv\")\n",
    "df.to_csv('data/txsmvccs_post.csv')\n",
    "print('mvcc causes found total: ' + str(df['mvccs_caused'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keys and the amount of MVCCs they caused\n",
    "A key causes an mvcc conflict if it is read with a version number, that is different than the one in the current state of the blockchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv containing the processed data\n",
    "post_process = pd.read_csv(\"data/txsmvccs_post.csv\")\n",
    "# plot the first 10 keys\n",
    "fig = px.bar(post_process.head(10), x='key', y='mvccs_caused',title=\"The number of MVCC conflicts caused by each key\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/txsmvccs_post_no_gb.csv')\n",
    "# group by keys and calculate the number of mvcc caused by each\n",
    "df = df.groupby(['key']).agg(mvcc_cause_for_tx_sum=('mvcc_cause_for_tx', 'sum')).reset_index()\n",
    "# sort rows by amount of mvcc caused descending (Z-A)\n",
    "post_process = df.sort_values(by=['mvcc_cause_for_tx_sum'], ascending=[False])\n",
    "print(\"Total number of mvccs found when limiting cause to 1/mvcc transaction: \" + str(post_process['mvcc_cause_for_tx_sum'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(post_process.head(10), x='key', y='mvcc_cause_for_tx_sum',title=\"The number of MVCC conflicts caused by each key, 1 cause/tx limit\")\n",
    "fig.update_yaxes(title=\"Quantity\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the processed dataframe containing the information about mvcc causing keys\n",
    "post_process = pd.read_csv(\"data/txsmvccs_post_no_gb.csv\")\n",
    "# select mvcc causing keys\n",
    "post_process = post_process[post_process[\"mvcc_cause\"]>0]\n",
    "# group data by the called function and calculate the sum mvccs caused by written keys in the function\n",
    "post_process = post_process.groupby(['called_function']).agg(mvccs_caused=('mvcc_cause_for_tx', 'sum')).reset_index()\n",
    "post_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amount of MVCCS each transaction caused\n",
    "With the help of the previous metric we can calculate the number of mvccs each transaction caused, by summing the previous number for all keys in a transaction's writeset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_1D(series):\n",
    "    return pd.Series([x for _list in series for x in _list])\n",
    "def getCausedAmount(keylist):\n",
    "    amount = 0\n",
    "    if not str(keylist)=='nan':\n",
    "        for key in keylist:\n",
    "            if(True):\n",
    "                try:\n",
    "                    amount += int(str(key).split('->')[1])\n",
    "                except IndexError:\n",
    "                    print(key)\n",
    "    return amount\n",
    "def getKeys(keylist):\n",
    "    keys=[]\n",
    "    if not str(keylist)=='nan':\n",
    "        for key in keylist:\n",
    "            if(len(key)>0):\n",
    "                keys.append(key.split('->')[0])\n",
    "    return keys\n",
    "# Step: Select columns\n",
    "transactions_tmp = transactions[['id', 'delta']]\n",
    "# Step: Rename column\n",
    "transactions_tmp = transactions_tmp.rename(columns={'id': 'txid'})\n",
    "post_process = pd.read_csv(\"data/txsmvccs_post_no_gb.csv\")\n",
    "post_process['key_to_num_mvcc'] = post_process['key'].astype('string') + '->' + post_process['mvcc_cause'].astype('string')\n",
    "# post_process['grade'] = post_process['validation_code'].apply(lambda x: gradeValidation(x))\n",
    "post_process = post_process.groupby(['txid','access_type'])['key_to_num_mvcc'].apply(list)\n",
    "post_process = post_process.reset_index(name ='keys_to_num_mvcc')\n",
    "# Pivot dataframe from long to wide format using the variable column 'access_type' and the value column 'keys'\n",
    "post_process = post_process.pivot(index='txid', columns='access_type', values='keys_to_num_mvcc').reset_index()\n",
    "post_process.columns.name = ''\n",
    "# Merge the dataframe with the dataframe containing all transactions and their grades on the txid \n",
    "post_process = pd.merge(post_process, transactions_tmp, on='txid', how='outer')\n",
    "# Cumulative sum of the grades\n",
    "post_process['delta_cumsum'] = post_process['delta'].cumsum()\n",
    "post_process = post_process[(post_process['READ'].isna()==False)|(post_process['WRITE'].isna()==False)]\n",
    "# Rename the columns\n",
    "post_process = post_process.rename(columns={'READ': 'read_keys','WRITE': 'written_keys'})\n",
    "post_process\n",
    "# Extract the amount of mvccs each written key caused and calculate the sum of them to get the number of mvccs each transaction caused\n",
    "post_process['mvccs_caused']=post_process['written_keys'].apply(lambda x: getCausedAmount(x))\n",
    "# Extract the written keys from the written_key->number_of_mvccs_caused \"datastructure\"\n",
    "post_process['written_keys']=post_process['written_keys'].apply(lambda x: getKeys(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(post_process.sort_values(by=['txid'], ascending=[True]), x='txid', y='mvccs_caused', title='Amount of MVCCs each transaction caused')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reusing the previous calculation, and the one where each transaction was graded (-1 if it is mvcc or +1 if it is valid), we can better visualize where each mvcc causing write is in relation to the transactions they caused the confilct in.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(post_process, x='txid', y='delta_cumsum', size='mvccs_caused', title='Cumsum( #valid-#mvcc ) transactions, mark sized by mvccs caused', hover_data=['written_keys'])\n",
    "fig.update_yaxes(title_text='cumsum( valid-mvcc )')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# copy the df with reads only\n",
    "distDf = txDf_reads.copy()\n",
    "# add a write distance column based on key version(block only) and current blockid\n",
    "distDf['last_write_dist']=distDf['blockid']-distDf['version_block']\n",
    "distDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean distance to last write of a key\n",
    "If the block version number of the read is subtracted from the blockid of the transaction, we get the distance to the last write performed on the key. If this is averaged over all reads of the key, we get a kind of \"hotness\" indicator. The less \"hot\" the key is, the less likely it is to cause an MVCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# group by keys and calculate the average distance to last write\n",
    "dist = distDf.groupby(['key']).agg(last_write_dist_mean=('last_write_dist', 'mean')).reset_index()\n",
    "# sort rows by average distance to last write ascending\n",
    "dist = dist.sort_values(by=['last_write_dist_mean'], ascending=[True])\n",
    "# plot the first 10 keys\n",
    "fig = px.bar(dist.head(10), x='key', y='last_write_dist_mean',title=\"Mean distance to last write\")\n",
    "#comment out this line to see the graph in log scale\n",
    "# fig.update_yaxes(type='log')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transactions/second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df with transactions and their creation dates\n",
    "tps = timeDf.drop(columns=[\"chaincodename\",\"status\",\"creator_msp_id\",])\n",
    "# sort rows by creation date ascending\n",
    "tps = tps.sort_values(by=['createdt'], ascending=[True])\n",
    "# reduce creation time resolution to seconds\n",
    "tps['createdt'] = pd.to_datetime(tps['createdt'].dt.strftime('%Y-%m-%d %H:%M:%S'),format='%Y-%m-%d %H:%M:%S')\n",
    "# calculate transactions created/second\n",
    "tps = tps.groupby(['createdt']).size()\n",
    "# plot the data\n",
    "fig = px.line(x=tps.index,y=tps.values,labels={'x':'Time','y':'Tps'})\n",
    "#comment out this line to see the graph in log scale\n",
    "# fig.update_yaxes(type='log')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General data about the transactions per second. \"Count\" here means the duration of the test, in seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tps.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transactions/second by validation code\n",
    "The same data, but transaction count/second is colored by validation code. It helps visualize the amounts relative to eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df with transactions and their creation dates\n",
    "tps = timeDf.drop(columns=[\"chaincodename\",\"status\",\"creator_msp_id\",])\n",
    "# reduce creation time resolution to seconds\n",
    "tps['createdt'] = pd.to_datetime(tps['createdt'].dt.strftime('%Y-%m-%d %H:%M:%S'),format='%Y-%m-%d %H:%M:%S')\n",
    "# group by createdt and validation code\n",
    "tps = tps.groupby(by=['createdt','validation_code'],as_index=False).size()\n",
    "# Pivot df from long to wide format using the variable column 'validation_code' and the value column 'size'\n",
    "tps = tps.pivot(index='createdt', columns='validation_code', values='size').reset_index()\n",
    "# fill nan values with zeros\n",
    "tps = tps.fillna(0)\n",
    "# melt back to original shape\n",
    "tps = tps.melt(id_vars=['createdt'],var_name='validation_code')\n",
    "# sort rows by createdt ascending (A-Z)\n",
    "tps = tps.sort_values(by=['createdt'], ascending=[True])\n",
    "# plot the data\n",
    "fig = px.area(tps, x='createdt', y='value', color='validation_code', color_discrete_sequence=px.colors.qualitative.G10[::-1])\n",
    "fig.update_xaxes(title_text='Time')\n",
    "fig.update_yaxes(title_text='Tx/sec')\n",
    "#comment out this line to see the graph in log scale\n",
    "# fig.update_yaxes(type='log')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number MVCCs/second\n",
    "This graph shows how many transactions are validated with the code MVCC_READ_CONFLICT each second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df with transactions and their creation dates\n",
    "mvccps = timeDf.drop(columns=[\"chaincodename\",\"status\",\"creator_msp_id\",])\n",
    "# sort rows by creation date ascending\n",
    "mvccps = mvccps.sort_values(by=['createdt'], ascending=[True])\n",
    "# reduce creation time resolution to seconds\n",
    "mvccps['createdt'] = pd.to_datetime(mvccps['createdt'].dt.strftime('%Y-%m-%d %H:%M:%S'),format='%Y-%m-%d %H:%M:%S')\n",
    "# keep transactions with validation code MVCC_READ_CONFLICT\n",
    "mvccps = mvccps.loc[mvccps['validation_code'].isin(['MVCC_READ_CONFLICT'])]\n",
    "# calculate number of mvcc transactions created/second\n",
    "mvccps = mvccps.groupby(['createdt']).size()\n",
    "# plot the data\n",
    "fig = px.line(x=mvccps.index,y=mvccps.values,labels={'x':'Time','y':'MVCCS/sec'})\n",
    "#comment out this line to see the graph in log scale\n",
    "# fig.update_yaxes(type='log')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read keys and the number of valid and mvcc transactions they occurred in\n",
    "A key that was read as part of a transaction marked with mvcc might not be the cause of the mvcc, but can reveal patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group reads by keys and validation code, and calculate the size of each group\n",
    "keycodedf = txDf_reads.groupby(by=['key','validation_code'],as_index=False).size()\n",
    "# Pivot dataframe from long to wide format using the variable column 'validation_code' and the value column 'size'\n",
    "# this is done so the data can be sorted by mvccs descending\n",
    "keycodedf = keycodedf.pivot(index='key', columns='validation_code', values='size').reset_index()\n",
    "# drop endorsement policy failure column\n",
    "if 'ENDORSEMENT_POLICY_FAILURE' in keycodedf.columns:\n",
    "    keycodedf = keycodedf.drop(columns=['ENDORSEMENT_POLICY_FAILURE'])\n",
    "# sort by MVCC_READ_CONFLICT descending, keep the first 10\n",
    "keycodedf = keycodedf.sort_values(by=['MVCC_READ_CONFLICT'], ascending=[False]).head(10)\n",
    "# return to original shape so it can be plotted\n",
    "keycodedf = keycodedf.melt(id_vars=['key'],var_name='validation_code')\n",
    "# fill nans with 0, as they were before transformations\n",
    "keycodedf = keycodedf.fillna(0)\n",
    "# plot the data\n",
    "labels={\"key\":\"Key\",\"value\":\"Occurrence in transactions\"}\n",
    "fig = px.bar(keycodedf, x='key', y='value', color='validation_code', barmode='group',labels=labels)\n",
    "#comment out this line to see the graph in log scale\n",
    "# fig.update_yaxes(type='log')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keys and the ratio of valid/mvcc reads done on them.\n",
    "If a key has a high ratio, it might be troublesome. It might not have caused the mvcc, but it might reveal patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previously created df with keys and their number of occurrences in each validation code group\n",
    "# drop the endorsement policy failure group\n",
    "ratio = txDf_reads.groupby(['key','validation_code']).size().reset_index().rename(columns={0:'size'})\n",
    "ratio = ratio.loc[~(ratio['validation_code'].isin(['ENDORSEMENT_POLICY_FAILURE']))]\n",
    "# pivot the table on validation codes\n",
    "ratio = ratio.pivot_table(index=['key'], \n",
    "            columns=['validation_code'], values='size').fillna(0)\n",
    "# calculate the ratio of the number of occurrences of each key in mvcc transactions/valid transactions\n",
    "ratio['ratio']=ratio['MVCC_READ_CONFLICT']/ratio['VALID']\n",
    "# drop rows where ratio is >= infinity  or <= 0\n",
    "ratio = ratio.loc[~((ratio['ratio'] >= np.inf) | (ratio['ratio'] <= 0))]\n",
    "# sort by the ratio descending\n",
    "ratio = ratio.reset_index()\n",
    "ratio = ratio.sort_values(by=['ratio'], ascending=[False])\n",
    "# plot the first 10\n",
    "fig = px.bar(ratio.head(10), x='key', y='ratio',title='Ratio of MVCC/Valid')\n",
    "#comment out this line to see the graph in log scale\n",
    "# fig.update_yaxes(type='log')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVCC analysis for each asset type\n",
    "Composite keys can be used in HLF to represent more complex data strutures. Below keys are grouped by asset types (the first part of the composite key by convention), and MVCC statistics are calculated for each type. This can show if a particular datatype is prone to be part of transactions with MVCC conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctxDf_reads=txDf_reads.copy()\n",
    "ctxDf_reads[\"asset_type\"]=ctxDf_reads[\"key\"].apply(lambda x: getAssetType(x))\n",
    "# Step: Group by asset_type, validation_code and calculate new column(s)\n",
    "valcodesperasset = ctxDf_reads.groupby(['asset_type', 'validation_code']).agg(size=('validation_code', 'size')).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation codes by asset type in read and write sets\n",
    "Not sure how useful this is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(valcodesperasset, x='asset_type', y='size', facet_row='validation_code', facet_row_spacing=0.35,title=\"Validation code by asset type in read sets\")\n",
    "#comment out this line to see the graph in linear scale\n",
    "fig.update_yaxes(type='log')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctxD_reads=txD_reads.copy()\n",
    "ctxD_reads[\"asset_type\"]=ctxD_reads[\"key\"].apply(lambda x: getAssetType(x))\n",
    "# Step: Group by asset_type, validation_code and calculate new column(s)\n",
    "valcodesperasset = ctxD_reads.groupby(['asset_type', 'validation_code']).agg(amount=('validation_code', 'size')).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(valcodesperasset, x='asset_type', y='amount', facet_row='validation_code', facet_row_spacing=0.35,title=\"Validation code by asset type in write sets\")\n",
    "#comment out this line to see the graph in linear scale\n",
    "fig.update_yaxes(type='log')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ctxDf_reads.copy()\n",
    "# group by asset type and validation code and calculate the size of each group\n",
    "df = df.groupby(['asset_type', 'validation_code']).agg(validation_code_size=('validation_code', 'size')).reset_index()\n",
    "ratio = df.loc[~(df['validation_code'].isin(['ENDORSEMENT_POLICY_FAILURE']))]\n",
    "ratio = ratio.pivot_table(index=['asset_type'], \n",
    "            columns=['validation_code'], values='validation_code_size').fillna(0)\n",
    "# calculate the ratio of the number of occurrences of each key in mvcc transactions/valid transactions\n",
    "ratio['ratio']=ratio['MVCC_READ_CONFLICT']/ratio['VALID']\n",
    "# drop rows where ratio is >= infinity  or <= 0\n",
    "ratio = ratio.loc[(~(ratio['ratio'] >= np.inf) | (ratio['ratio'] <= 0))]\n",
    "# sort by the ratio descending\n",
    "ratio = ratio.reset_index()\n",
    "ratio = ratio.sort_values(by=['ratio'], ascending=[False])\n",
    "ratio\n",
    "# plot the first 10\n",
    "fig = px.bar(ratio.head(10), x='asset_type', y='ratio',title='Ratio of MVCC/Valid by asset types in reads')\n",
    "#comment out this line to see the graph in log scale\n",
    "# fig.update_yaxes(type='log')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ctxD_reads.copy()\n",
    "# group by asset type and validation code and calculate the size of each group\n",
    "df = df.groupby(['asset_type', 'validation_code']).agg(validation_code_size=('validation_code', 'size')).reset_index()\n",
    "ratio = df.loc[~(df['validation_code'].isin(['ENDORSEMENT_POLICY_FAILURE']))]\n",
    "ratio = ratio.pivot_table(index=['asset_type'], \n",
    "            columns=['validation_code'], values='validation_code_size').fillna(0)\n",
    "# calculate the ratio of the number of occurrences of each key in mvcc transactions/valid transactions\n",
    "ratio['ratio']=ratio['MVCC_READ_CONFLICT']/ratio['VALID']\n",
    "# drop rows where ratio is >= infinity  or <= 0\n",
    "ratio = ratio.loc[~((ratio['ratio'] >= np.inf) | (ratio['ratio'] <= 0))]\n",
    "# sort by the ratio descending\n",
    "ratio = ratio.reset_index()\n",
    "ratio = ratio.sort_values(by=['ratio'], ascending=[False])\n",
    "ratio\n",
    "# plot the first 10\n",
    "fig = px.bar(ratio.head(10), x='asset_type', y='ratio',title='Ratio of MVCC/Valid by asset types in writes')\n",
    "# comment out this line to see the graph in log scale\n",
    "# fig.update_yaxes(type='log')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum of MVCCs caused by each asset type\n",
    "Because we previously calculated the amount of mvccs each write caused, now these keys can be grouped by asset types and the mvcc amount can be summed. This will show if a particular asset type is more problematic and might require chaincode changes to improve the goodput of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvccs_by_type = pd.read_csv(\"data/txsmvccs_post.csv\")\n",
    "# get the asset types from the keys\n",
    "mvccs_by_type[\"asset_type\"]=mvccs_by_type['key'].apply(lambda x: getAssetType(x))\n",
    "# Change data type of asset_type to String/Text for groupby to work correctly\n",
    "mvccs_by_type['asset_type'] = mvccs_by_type['asset_type'].astype('string')\n",
    "# Group by asset types and calculate the sum of mvccs they each caused\n",
    "mvccs_by_type = mvccs_by_type.groupby(['asset_type']).agg(mvccs_caused_sum=('mvccs_caused', 'sum')).reset_index()\n",
    "# Sort rows by mvccs_caused_sum ascending (A-Z)\n",
    "mvccs_by_type = mvccs_by_type.sort_values(by=['mvccs_caused_sum'], ascending=[False])\n",
    "fig = px.pie(mvccs_by_type.head(10), names='asset_type', values='mvccs_caused_sum', title='Percentage of total MVCCs caused by asset type')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(mvccs_by_type, x='asset_type', y='mvccs_caused_sum',title=\"Total number of MVCCs caused by asset type\")\n",
    "fig.update_xaxes(title_text='Asset type')\n",
    "fig.update_yaxes(title_text='Total mvccs caused')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all user defined tables in the explorer db\n",
    "query = \"SELECT * FROM pg_catalog.pg_tables WHERE schemaname != 'information_schema' AND schemaname != 'pg_catalog';\"\n",
    "idkdf = pd.read_sql(query,con=engine)\n",
    "idkdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/txsmvccs_post_no_gb.csv')\n",
    "# Step: Keep rows where (validation_code is one of: MVCC_READ_CONFLICT) and (mvcc_cause_found_for_tx is False)\n",
    "df = df.loc[(df['validation_code'].isin(['MVCC_READ_CONFLICT'])) & (df['mvcc_cause_found_for_tx'] == False) & (df['access_type']=='READ') & (df['version_block']>df['blockid'].min())]\n",
    "\n",
    "# Step: Group by and aggregate\n",
    "df = df.groupby(['txid']).agg(txid_size=('txid', 'size')).reset_index()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/txsmvccs_post_no_gb.csv')\n",
    "# group by keys and calculate the number of mvcc caused by each\n",
    "df = df.groupby(['key']).agg(mvcc_cause_for_tx_sum=('mvcc_cause_for_tx', 'sum')).reset_index()\n",
    "# sort rows by amount of mvcc caused descending (Z-A)\n",
    "types = df.sort_values(by=['mvcc_cause_for_tx_sum'], ascending=[False])\n",
    "types['asset_type']=types['key'].apply(lambda x:getAssetType(x))\n",
    "types = types.groupby(['asset_type']).agg(mvcc_cause_for_tx_sum=('mvcc_cause_for_tx_sum', 'sum')).reset_index()\n",
    "types = types.loc[types['mvcc_cause_for_tx_sum']>0].sort_values(by='mvcc_cause_for_tx_sum',ascending=False)\n",
    "fig = px.pie(types.head(10), values='mvcc_cause_for_tx_sum', names='asset_type',title='Percentage of total MVCCs caused by asset type, 1 cause/tx limit')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(types, x='asset_type', y='mvcc_cause_for_tx_sum', title='MVCC conflicts caused by each asset type, 1 cause/tx limit')\n",
    "fig.update_yaxes(title_text='Quantity')\n",
    "fig.update_xaxes(title_text='Asset type')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/txsmvccs_post_no_gb.csv')\n",
    "# Step: Keep rows where (validation_code is one of: MVCC_READ_CONFLICT) and (mvcc_cause_found_for_tx is False)\n",
    "df = df.loc[(df['validation_code'].isin(['MVCC_READ_CONFLICT'])) & (df['mvcc_cause_found_for_tx'] == False)]\n",
    "# Step: Group by and aggregate\n",
    "df = df.groupby(['txid']).agg(key_size=('key', 'size')).reset_index().rename(columns={'key_size':'keyaccesses'})\n",
    "print('MVCC transactions for which no cause is found: ' + str(len(df)))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/txsmvccs_post_no_gb.csv')\n",
    "df['asset_type'] = df['key'].apply(lambda x: getAssetType(x))\n",
    "df = df.groupby(['asset_type','mvcc_cause_for_tx']).size().reset_index().rename(columns={0:'size'})\n",
    "df.set_index('asset_type',inplace=True)\n",
    "causers = df['mvcc_cause_for_tx']*df['size']\n",
    "causers = causers.groupby('asset_type').sum()\n",
    "causers = causers.loc[causers > 0]\n",
    "df = df.loc[df.index.isin(causers.index)]\n",
    "df = df.loc[df.mvcc_cause_for_tx>0]\n",
    "fig = px.histogram(df, x='mvcc_cause_for_tx', y='size', facet_row=df.index, nbins=df['mvcc_cause_for_tx'].max(), facet_row_spacing=0.08, \n",
    "                   height=len(causers)*250, title='Distribution of the number of MVCC conflicts caused by asset type per transaction')\n",
    "fig.update_xaxes(title_text='Number of conflicts caused per transaction')\n",
    "fig.update_yaxes(title_text='Quantity of txs')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/txsmvccs_post_no_gb.csv')\n",
    "df = df.loc[~df['mvcc_caused_at_for_tx'].isna()]\n",
    "mvcc_txs = df['mvcc_caused_at_for_tx'].str.split(',').apply(lambda x: x[:-1])\n",
    "mvcc_txs = np.concatenate(mvcc_txs.values).ravel().astype(int)\n",
    "mvcc_txs = transactions.loc[transactions['id'].isin(mvcc_txs)]\n",
    "mvcc_txs = mvcc_txs[['id','blockid','createdt', 'validation_code', 'read_keys', 'written_keys']]\n",
    "mvcc_txs.to_csv('data/mvcc_txs.csv')\n",
    "df = df[['txid','blockid','key','validation_code', 'version_block','version_tx','called_function','mvcc_cause_for_tx', 'mvcc_caused_at_for_tx']]\n",
    "# df['mvcc_caused_at_for_tx'] = df['mvcc_caused_at_for_tx'].str.split(',').apply(lambda x: x[:-1])\n",
    "df.to_csv('data/mvcc_keys.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network analysis\n",
    "Transactions or keys can be displayed as Nodes and different relationships between them can be represented with edges to gain more insight about the data access conflicts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database setup\n",
    "The following cells initialize the neo4j database that was started with the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleteGraphDbContent()\n",
    "\n",
    "## load mvcc keys to graph database\n",
    "gds = GraphDataScience(\"bolt://localhost:7687\", auth=None)\n",
    "queryres = gds.run_cypher(\n",
    "\"\"\"\n",
    "LOAD CSV WITH HEADERS FROM 'file:///mvcc_keys.csv'\n",
    "    AS row\n",
    "    CREATE (k:Key{\n",
    "        txid:row.txid,\n",
    "        blockid:row.blockid,\n",
    "        key:row.key,\n",
    "        validation_code:row.validation_code,\n",
    "        version_block:row.version_block,\n",
    "        version_tx:row.version_tx,\n",
    "        called_function:row.called_function,\n",
    "        mvcc_cause:row.mvcc_cause_for_tx,\n",
    "        mvcc_caused_at:row.mvcc_caused_at_for_tx\n",
    "        })\n",
    "\"\"\"\n",
    ")\n",
    "queryres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load transactions affected by mvcc conflicts into neo4j\n",
    "gds = GraphDataScience(\"bolt://localhost:7687\", auth=None)\n",
    "queryres = gds.run_cypher(\n",
    "\"\"\"\n",
    "LOAD CSV WITH HEADERS FROM 'file:///mvcc_txs.csv'\n",
    "    AS row\n",
    "    CREATE (tx:Transaction{\n",
    "        txid:row.id,\n",
    "        blockid:row.blockid,\n",
    "        createdt:row.createdt,\n",
    "        read_keys:row.read_keys,\n",
    "        written_keys:row.written_keys\n",
    "        })\n",
    "\"\"\"\n",
    ")\n",
    "queryres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform array like list into array\n",
    "gds = GraphDataScience(\"bolt://localhost:7687\", auth=None)\n",
    "queryres = gds.run_cypher(\n",
    "\"\"\"\n",
    "MATCH (k:Key) \n",
    "SET k.mvcc_caused_at = tail(reverse(split(k.mvcc_caused_at,','))) \n",
    "RETURN k\n",
    "\"\"\"\n",
    ")\n",
    "queryres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create relationships between the mvcc causing keys and the transactions\n",
    "gds = GraphDataScience(\"bolt://localhost:7687\", auth=None)\n",
    "queryres = gds.run_cypher(\n",
    "\"\"\"\n",
    "MATCH (k:Key)\n",
    "UNWIND k.mvcc_caused_at AS txid \n",
    "MATCH (tx:Transaction) \n",
    "WHERE tx.txid = txid \n",
    "CREATE (k)-[r:CAUSES_MVCC]->(tx) \n",
    "RETURN type(r)\n",
    "\"\"\"\n",
    ")\n",
    "queryres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post initialization\n",
    "The database has been initialized if you have run the couple cells above this. To view the graph, open this [link](http://localhost:7474/browser/), select \"No Authentication\" and connect. After this you will be able to query the database with cypher, the query language of neo4j. To see the whole graph (max 300 nodes are displayed by default) run the following query:\n",
    "```\n",
    "MATCH (k:Key)-[r:CAUSES_MVCC]->(tx:Transaction) \n",
    "RETURN k, tx, COUNT(r) as n \n",
    "ORDER BY n DESC\n",
    "```\n",
    "This will display a graph with 2 types of nodes: key, transaction. The keys are written keys that caused mvcc conflicts, while the transactions are all invalidated ones with the code MVCC_READ_CONFLICT. The keys are connected by directed edges to the transactions that are invalidated because of them. If a node is clicked on, additional information can be seen on the right hand side panel. As mentioned above, the graph browser will only display 300 nodes by default. For this reason the results are ordered by the number of CAUSES_MVCC relationships in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleteGraphDbContent()\n",
    "## load mvcc keys to graph database\n",
    "gds = GraphDataScience(\"bolt://localhost:7687\", auth=None)\n",
    "df = pd.read_csv('data/txsmvccs_post_no_gb.csv')\n",
    "# print(df.isnull().any())\n",
    "df = df.fillna('-',axis=1)\n",
    "# print(df.isnull().any())\n",
    "df.to_csv('data/txsmvccs_post_no_gb_no_null.csv')\n",
    "queryres = gds.run_cypher(\n",
    "\"\"\"\n",
    "LOAD CSV WITH HEADERS FROM 'file:///txsmvccs_post_no_gb_no_null.csv'\n",
    "    AS row\n",
    "    CREATE (k:Key{\n",
    "        txid:row.txid,\n",
    "        blockid:row.blockid,\n",
    "        key:row.key,\n",
    "        validation_code:row.validation_code,\n",
    "        version_block:row.version_block,\n",
    "        version_tx:row.version_tx,\n",
    "        access_type:row.access_type,\n",
    "        called_function:row.called_function,\n",
    "        mvcc_cause:row.mvcc_cause_for_tx,\n",
    "        mvcc_caused_at:row.mvcc_caused_at_for_tx\n",
    "        })\n",
    "\"\"\"\n",
    ")\n",
    "queryres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryres = gds.run_cypher(\n",
    "\"\"\"\n",
    "MATCH (k:Key) MERGE (f:Function{name:k.called_function}) RETURN f\n",
    "\"\"\"\n",
    ")\n",
    "queryres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryres = gds.run_cypher(\n",
    "\"\"\"\n",
    "MATCH (f:Function), (k:Key)\n",
    "WHERE f.name = k.called_function\n",
    "AND k.access_type = \"READ\"\n",
    "CREATE (f)-[r:READS{version_block:k.version_block,version_tx:k.version_tx}]->(k)\n",
    "RETURN f,r,k\n",
    "\"\"\"\n",
    ")\n",
    "queryres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryres = gds.run_cypher(\n",
    "\"\"\"\n",
    "MATCH (f:Function), (k:Key)\n",
    "WHERE f.name = k.called_function\n",
    "AND k.access_type = \"WRITE\"\n",
    "CREATE (f)-[r:WRITES{version_block:k.version_block,version_tx:k.version_tx}]->(k)\n",
    "RETURN f,r,k\n",
    "\"\"\"\n",
    ")\n",
    "queryres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryres = gds.run_cypher(\n",
    "\"\"\"\n",
    "MATCH (k:Key)\n",
    "MERGE (a:AssetType{type: split(k.key,\"||\")[1]})\n",
    "RETURN a\n",
    "\"\"\"\n",
    ")\n",
    "queryres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryres = gds.run_cypher(\n",
    "\"\"\"\n",
    "MATCH (k:Key),(a:AssetType)\n",
    "WHERE split(k.key,\"||\")[1] = a.type\n",
    "CREATE (k)-[r:IS_OF_TYPE]->(a)\n",
    "RETURN type(r)\n",
    "\"\"\"\n",
    ")\n",
    "queryres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryres = gds.run_cypher(\n",
    "\"\"\"\n",
    "MATCH (f:Function)-[r:READS]->(k:Key)-[IS_OF_TYPE]->(a:AssetType)\n",
    "MERGE (f)-[rt:READS_TYPE]->(a)\n",
    "RETURN type(rt), f.name,k.key,a.type\n",
    "\"\"\"\n",
    ")\n",
    "queryres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryres = gds.run_cypher(\n",
    "\"\"\"\n",
    "MATCH (f:Function)-[r:WRITES]->(k:Key)-[IS_OF_TYPE]->(a:AssetType)\n",
    "MERGE (f)-[wt:WRITES_TYPE]->(a)\n",
    "RETURN type(wt), f.name,k.key,a.type\n",
    "\"\"\"\n",
    ")\n",
    "queryres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryres = gds.run_cypher(\n",
    "\"\"\"\n",
    "MATCH (f:Function)-[r:READS_TYPE|WRITES_TYPE]->(a:AssetType)\n",
    "RETURN f.name,type(r), a.type\n",
    "\"\"\"\n",
    ")\n",
    "queryres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/txsmvccs_post_no_gb.csv')\n",
    "df1 = df1.loc[df1['mvcc_cause_for_tx']>0]\n",
    "df1 = df1.groupby(['called_function']).agg(mvccs_caused=('mvcc_cause_for_tx', 'sum')).reset_index()\n",
    "df2 = pd.read_csv('data/txsmvccs_post_no_gb.csv')\n",
    "df2 = df2.loc[df2['validation_code']=='MVCC_READ_CONFLICT']\n",
    "df2 = df2.groupby(['txid','called_function']).size().groupby('called_function').size().reset_index().rename(columns={0:'invalidated_with_mvcc'})\n",
    "df = pd.merge(df1,df2,on='called_function',how='outer').fillna(0,axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.system(\"rm mvcc-eda.html\")\n",
    "# os.system(\"jupyter nbconvert mvcc-eda.ipynb --to html\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
